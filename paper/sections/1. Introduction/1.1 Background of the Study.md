# 1. Introduction

## 1.1 Background
Chatbots are increasingly being adopted across various industries to enhance customer service and operational efficiency, with the restaurant sector providing a clear example. For instance, (Gupta, Dheekonda, & Masum, 2024) introduced a chatbot named Genie, which efficiently handles customer inquiries about reservations, menus, and frequently asked questions using predefined responses. While this rule-based approach has proven effective for routine queries, it lacks the flexibility and depth of understanding that large language models (LLMs) offer. However, LLM-based chatbots face a significant limitation: once trained, they cannot automatically incorporate new information and must be retrained with updated datasets to remain accurate and relevant—a process that is both time-consuming and computationally expensive. This challenge can be addressed through Retrieval-Augmented Generation (RAG), an approach that allows LLMs to dynamically retrieve and integrate the most current information from external sources, enabling more adaptive, informative, and contextually relevant responses without the need for constant retraining.

This capability is especially beneficial in industries where information frequently changes and must be tailored to individual cases, such as travel agencies in the Philippines that assist clients with obtaining Japan visas. These agencies manage a high volume of inquiries related to the required documents for different visa types, including tourist, business, and family-related visits. While (Embassy of Japan in the Philippines, 2025) provides general visa requirements on its website, these guidelines are often not exhaustive. Many supporting documents—especially those that serve as proof of the applicant’s specific purpose for visiting—are not publicly listed but are essential for successful applications. For example, if the applicant is visiting Japan to care for a child, they must submit a baby book; if the visit is for medical reasons, a medical certificate is typically required. These specialized documents vary depending on the applicant’s situation and are typically known only to travel agencies, who stay informed through frequent updates and experience.

As a result, clients often seek clarification before visiting the agency to avoid delays or multiple trips due to incomplete submissions. A large number of these inquiries are made via messaging platforms, particularly Facebook Messenger, which is commonly used in the Philippines for casual and business communication. While convenient, manually responding to each message consumes time and resources, especially when questions are repetitive or case-specific. Implementing a RAG-based chatbot system within these messaging platforms would allow travel agencies to automatically provide real-time, accurate, and context-aware answers to client inquiries. This solution would not only reduce the workload of staff but also improve client preparedness, enhance satisfaction, and streamline the overall Japan visa application process.


### Methods of Building a Chatbot

Early chatbot systems, such as ELIZA and ALICE, employ a rule-based approach that uses predefined set of rules to generate responses. These systems typically rely on pattern-matching algorithms to identify the most appropriate response based on user queries. While pattern matching enables chatbots to accommodate variations in user input without the need to manually code each possible response. The problem with template-based is it does not build knowledge base, (Thorat & Jadhav, 2020) this limitation can lead to difficulties; any change in the chatbot's requirements means manually updating the template to adapt to these changes. When data is small, minor adjustments may be manageable but when handling many required responses can be quite challenging. (Formica, Mele, & Taglino, 2024) address these limitations by introducing a hybrid method rule-based and machine learning methods. In this framework, human experts create the query templates, while the classification and query processing steps are managed by a machine learning model. Statistical-based chatbots are systems that generate responses by using machine learning and Natural Language Processing methods (NLP), which trains on large datasets, or corpora, of textual data. 

Statistical-based chatbots are systems that generate responses using machine learning and Natural Language Processing (NLP) techniques, relying on large datasets, or corpora, of textual data for training. Due to the vast amount of data involved, a solid understanding of data cleaning and preprocessing is essential. As described by (Kalra & Aggarwal, 2017), the common steps for data cleaning and preprocessing are:

1.	Removal: Strip away irrelevant content, such as URLs, special characters, and HTML tags.
2.	Deduplication: Remove duplicate entries to ensure the dataset is concise and free of redundancy.
3.	Text Normalization: Standardize the text by converting it to lowercase, expanding contractions, and correcting spelling errors.
4.	Stop word Removal: Eliminate common words that don’t add significant meaning, like "the" or "is."
5.	Tokenization: Split the text into meaningful units, such as words or phrases.
6.	Lemmatization/Stemming: Reduce words to their base or root form to address variations in word forms.
These methods serve as guidelines when dealing with text data across various branches of machine learning, such as deep learning, reinforcement learning, and more.

Deep learning is a specific machine learning field that is based on building artificial neural networks and creating architecture such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks. These laid the groundwork for the development of Transformer-based models, including Large Language Models (LLMs), which continue to be an active area of research. (Sharma & Gupta, 2018) provides an example of chatbot implementation involves the integration of both NLP techniques and deep learning architectures. In this approach, words are transformed into vector representations, which are then fed into models like LSTM to enable learning from question-answer pairs. This process allows the chatbot to generate more accurate and contextually relevant responses. The difference between building a knowledge base between traditional matching learning methods and deep learning is that it requires large amount of data to train.


### Knowledge Base

This section explores various methods for building a knowledge base, which is a crucial component in enhancing the performance of large language models (LLMs). A knowledge base serves as a repository that stores information such as text, images and audio which can be used to provide accurate answers to users. 

Two primary approaches to building a knowledge base in this context are identified. The first approach, as discussed by (Singh & Solanki, 2016), involves using relational databases (SQL). This method encompasses the storage of raw data and the application of Natural Language Processing (NLP) techniques for data preprocessing. Key steps in this process include converting text to lowercase, tokenization, removing stop words, and applying word embeddings to prepare the data for model training. While this approach allows for the use of various machine learning and deep learning methods, it poses challenges. SQL databases are not optimized for storing and retrieving vectors, which can lead to performance issues during vector operations. Additionally, scaling SQL databases to handle large datasets can be problematic.

The second approach, highlighted by (Luo, Lau, Li, & Si, 2022), involves utilizing a vector database, which is specifically designed for efficient storage and retrieval of vectors and includes built-in functionalities for such operations. An example of a NoSQL vector database is Apache Cassandra, which employs its own SQL variant known as Cassandra Query Language (CQL) with features optimized for vector operations. However, a drawback of this approach is its complexity, as users must possess specialized knowledge to operate and maintain the vector database effectively.

Lastly, (Nazir, Khan, Ahmed, & Wasi, 2019) propose an alternative technique for storing and organizing questions and answers using an ontology model. This method has been successfully implemented in the development of a chatbot capable of responding to consumer inquiries about apparel brands and products. One notable advantage of this ontology-based approach is its flexibility in knowledge management.


### Retrieval Augmented Generation

(Gao, et al., 2023) Discusses the three fundamental components for implementing Retrieval-Augmented Generation (RAG). These components serve as the foundation for various methodologies, including Naive, Advanced, and Modular approaches, each offering distinct advantages and techniques for effectively handling and generating information.

Retrieval-Augmented Generation (RAG) frameworks have become much more popular in recent years. Developments, according to (Wu, 2024) The two main phases of these frameworks are retrieval and generation. Sparse retrieval and dense retrieval are the two primary methods used in the retrieval phase. Term frequency and inverse document frequency (TF-IDF) are used by sparse retrieval techniques like BM25 to evaluate the relevance of documents. The limited capacity of sparse retrieval systems to capture semantic relationships makes them less effective when handling complex queries that demand a deeper contextual understanding, despite their computational efficiency and suitability for real-time applications (Read, 2024). On the other hand, transformer models are used to describe queries and documents as dense vectors in dense retrieval approaches, such as Dense Passage Retrieval (DPR). They are especially well-suited to managing complex queries since this enables improved context-aware retrieval. However, real-time applications or systems with limited processing power may find dense retrieval approaches problematic due to their increased computing needs (Wu, 2024)

RAG-Token and RAG-Sequence are two important methods used in the generation phase. Using context from retrieved documents, RAG-Token produces replies progressively, token by token. This method balances response fluency and retrieval accuracy, making it adaptable to a range of tasks. The token-by-token method, however, can lead to less than ideal coherence, particularly if the contextual data from the texts that were obtained is insufficient (Yuan, 2024) RAG-Sequence, on the other hand, ensures globally consistent and contextually rich outputs by using the retrieved documents to construct a comprehensive response. This approach may not be appropriate for real-time applications due to its slower processing speeds and higher computational overhead, even though it maximizes the use of recovered context (Wu, 2024) In the end, the particular application needs will determine whether to use sparse or dense retrieval techniques, as well as RAG-Token or RAG-Sequence. In tasks where speed and efficiency are important, sparse retrieval methods like BM25 might be enough. However, when combined with RAG-Sequence, dense retrieval techniques like DPR may provide superior performance for tasks requiring a deeper comprehension of semantic links and context, albeit at the expense of greater computational complexity. According to recent research, by integrating external knowledge bases, RAG frameworks can improve the accuracy and dependability of generated content by addressing typical problems such hallucinations and knowledge updates in big language models (Wu, 2024). As research continues to evolve, efforts are being made to optimize the balance between efficiency and effectiveness in RAG systems, particularly in AI-driven content generation (Yuan, 2024)