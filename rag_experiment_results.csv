index_name,embeddings_model_name,llm_model_name,similarity_top_k,alpha,prompt_template,cross_encoder_model,rerank_top_n,llm_provider,faithfulness,factual_correctness(mode=f1),llm_context_precision_with_reference,context_recall,avg_latency_sec,latency_95th_sec
Requirements,intfloat/multilingual-e5-base,gemma3:1b,3,0.8,"metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['query_str', 'context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are a helpful assistant that answers Japan visa questions.\n\nQuestion: {query_str}\n\nHere are the retrieved documents:\n{context_str}\n\nAnswer clearly and concisely.'",cross-encoder/ms-marco-MiniLM-L-2-v2,1,ollama,1.0,0.27,0.9999999999,0.7333333333333333,63.1849262714386,63.1849262714386
Requirements,intfloat/multilingual-e5-base,gemma3:1b,3,0.8,"metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['query_str', 'context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are a helpful assistant that answers Japan visa questions.\n\nQuestion: {query_str}\n\nHere are the retrieved documents:\n{context_str}\n\nAnswer clearly and concisely.'",cross-encoder/ms-marco-MiniLM-L-2-v2,1,ollama,0.9473684210526316,0.23,0.9999999999,0.7142857142857143,62.97425293922424,62.97425293922424
Requirements,intfloat/multilingual-e5-base,gemma3:1b,3,0.8,"metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['query_str', 'context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are a helpful assistant that answers Japan visa questions.\n\nQuestion: {query_str}\n\nHere are the retrieved documents:\n{context_str}\n\nAnswer clearly and concisely.'",cross-encoder/ms-marco-MiniLM-L-2-v2,1,ollama,0.8888888888888888,0.47,0.9999999999,0.7333333333333333,70.7661280632019,70.7661280632019
