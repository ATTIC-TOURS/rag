{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7efe72",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "In chatbots or RAG systems, it's often used in:\n",
    "\n",
    "- Intent classification\n",
    "\n",
    "- Document retrieval\n",
    "\n",
    "- Keyword extraction\n",
    "\n",
    "📈 Use Cases of High TF-IDF Terms:\n",
    "- Pick keywords for summaries\n",
    "\n",
    "- Rank documents in search results\n",
    "\n",
    "- Retrieve relevant context in RAG\n",
    "\n",
    "- Feature selection for ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa8c16",
   "metadata": {},
   "source": [
    "## TF (Term Frequency)\n",
    "\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "db796546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(term, doc):\n",
    "    return doc.lower().split().count(term.lower()) / len(doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0e2ad4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of I in\n",
      "\"I am kenji\"\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "term = \"I\"\n",
    "doc = \"I am kenji\"\n",
    "\n",
    "result = compute_tf(term, doc)\n",
    "\n",
    "print(f'Term Frequency of {term} in\\n\"{doc}\"\\n{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda72564",
   "metadata": {},
   "source": [
    "## IDF (Inverse Document Frequency)\n",
    "IDF reduces the weight of common words and increases the weight of **rare**, informative ones.\n",
    "\n",
    "$$\n",
    "IDF(t) = \\log \\left( \\frac{N}{df(t)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N = \\text{total number of documents}$\n",
    "- $df(t) = \\text{number of documents containing term t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b773b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_idf(term, docs, smooth=True):\n",
    "    doc_count = sum(term.lower() in doc.lower().split() for doc in docs)\n",
    "    if smooth:\n",
    "        return math.log((1 + len(docs)) / (1 + doc_count))\n",
    "    \n",
    "    if doc_count == 0:\n",
    "        return 0\n",
    "    \n",
    "    return math.log(len(docs) / doc_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5c15d173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"I want to visit Japan for tourism. What kind of visa do I need?\",\n",
    "    \"Can I apply for a working holiday visa if I'm from Australia?\",\n",
    "    \"I have a Japanese spouse. How do I apply for a spouse visa?\",\n",
    "    \"What documents are needed to get a short-term business visa for Japan?\",\n",
    "    \"I’m attending a 2-week academic conference in Tokyo. Which visa is suitable?\",\n",
    "    \"How long can I stay in Japan on a tourist visa?\",\n",
    "    \"Do I need a visa if I’m transiting through Narita Airport for 5 hours?\",\n",
    "    \"I'm planning to study in Japan for one year. What visa do I need?\",\n",
    "    \"What is the difference between a temporary visitor visa and a multiple-entry visa?\",\n",
    "    \"Can I convert a tourist visa into a student visa while in Japan?\"\n",
    "]\n",
    "\n",
    "term = \"fasddsaf\"\n",
    "result = compute_idf(term, docs, smooth=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48299360",
   "metadata": {},
   "source": [
    "## Smoothed IDF\n",
    "\n",
    "$$\n",
    "IDF(t) = \\log \\left( \\frac{1+N}{1+df(t)} \\right)\n",
    "$$\n",
    "\n",
    "✅ This ensures:\n",
    "\n",
    "- The denominator is never zero\n",
    "\n",
    "- The value is finite and stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "78db7214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3978952727983707"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = compute_idf(\"2-wesfdek\", docs, smooth=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331ee38",
   "metadata": {},
   "source": [
    "## unsmoothed vs smoothed IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4d32ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: 2.302585092994046\n",
      "smoothed: 1.7047480922384253\n"
     ]
    }
   ],
   "source": [
    "term = \"japanese\"\n",
    "idf_result = compute_idf(term, docs, smooth=True)\n",
    "unsmoothed_idf_result = compute_idf(term, docs, smooth=False)\n",
    "\n",
    "print(f\"normal: {unsmoothed_idf_result}\")\n",
    "print(f\"smoothed: {idf_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb99a0",
   "metadata": {},
   "source": [
    "### TF-IDF (One Single Term)\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a851cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(term, doc, docs, smooth=True):\n",
    "    return compute_tf(term, doc) * compute_idf(term, docs, smooth=smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5e88f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a Japanese spouse. How do I apply for a spouse visa?\n",
      "tf-idf\n",
      "japanese: 0.13113446863372502\n"
     ]
    }
   ],
   "source": [
    "term = \"japanese\"\n",
    "target_doc = docs[2]\n",
    "\n",
    "result = compute_tfidf(term, target_doc, docs, smooth=True)\n",
    "\n",
    "print(target_doc)\n",
    "print(\"tf-idf\")\n",
    "print(f\"{term}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe29ef",
   "metadata": {},
   "source": [
    "## TF-IDF (Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "42c70b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(docs):\n",
    "    tokenized_docs = [doc.lower().split() for doc in docs]\n",
    "    return sorted(set(word.lower() for doc in tokenized_docs for word in doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "690f16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_matrix(docs):\n",
    "    tfidf_matrix = []\n",
    "    vocabulary = get_vocabulary(docs)\n",
    "    for doc in docs:\n",
    "        tfidf_vector = []\n",
    "        for term in vocabulary:\n",
    "            tf = compute_tf(term, doc)\n",
    "            idf = compute_idf(term, docs, smooth=True)\n",
    "            tfidf = tf * idf\n",
    "            tfidf_vector.append(tfidf)\n",
    "        tfidf_matrix.append(tfidf_vector)\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd7b6b",
   "metadata": {},
   "source": [
    "The following is to get the keyword per document using the tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "af522ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      "kind: 0.12176772087417323\n",
      "\n",
      "Document 2\n",
      "australia?: 0.14206234101986875\n",
      "\n",
      "Document 3\n",
      "have: 0.13113446863372502\n",
      "\n",
      "Document 4\n",
      "are: 0.14206234101986875\n",
      "\n",
      "Document 5\n",
      "2-week: 0.14206234101986875\n",
      "\n",
      "Document 6\n",
      "long: 0.15497709929440232\n",
      "\n",
      "Document 7\n",
      "5: 0.12176772087417323\n",
      "\n",
      "Document 8\n",
      "one: 0.12176772087417323\n",
      "\n",
      "Document 9\n",
      "and: 0.13113446863372502\n",
      "\n",
      "Document 10\n",
      "convert: 0.13113446863372502\n"
     ]
    }
   ],
   "source": [
    "tfidf_result = compute_tfidf_matrix(docs)\n",
    "vocabulary = get_vocabulary(docs)\n",
    "\n",
    "for i, row in enumerate(tfidf_result):\n",
    "    print(f\"\\nDocument {i+1}\")\n",
    "    keywords = []\n",
    "    for term, score in zip(vocabulary, row):\n",
    "        keywords.append((term, score))\n",
    "    keyword = max(keywords, key=lambda x: x[1])\n",
    "    print(f'{keyword[0]}: {keyword[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5dc37",
   "metadata": {},
   "source": [
    "## Learned\n",
    "\n",
    "1. Text Cleaning\n",
    "2. Tokenization\n",
    "3. Additive Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf63415",
   "metadata": {},
   "source": [
    "# Using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9165c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Using cached numpy-2.3.1-cp311-cp311-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.0-cp311-cp311-macosx_14_0_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-macosx_10_9_x86_64.whl (9.3 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached numpy-2.3.1-cp311-cp311-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "Using cached scipy-1.16.0-cp311-cp311-macosx_14_0_x86_64.whl (23.4 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 numpy-2.3.1 scikit-learn-1.7.1 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "836b8910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Document 1\n",
      "Top keyword: 'filipino' (score: 0.4936)\n",
      "\n",
      "📄 Document 2\n",
      "Top keyword: 'apply' (score: 0.4426)\n",
      "\n",
      "📄 Document 3\n",
      "Top keyword: 'and' (score: 0.4282)\n",
      "\n",
      "📄 Document 4\n",
      "Top keyword: 'difference' (score: 0.4037)\n",
      "\n",
      "📄 Document 5\n",
      "Top keyword: 'agencies' (score: 0.3663)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 📝 Sample corpus\n",
    "documents = [\n",
    "    \"Japan visa requirements for Filipino travelers.\",\n",
    "    \"How to apply for a Japan tourist visa?\",\n",
    "    \"Japan embassy visa checklist and required documents.\",\n",
    "    \"Schengen visa versus Japan visa: what's the difference?\",\n",
    "    \"Best travel agencies that assist with Japan visa application.\"\n",
    "]\n",
    "\n",
    "# 🛠️ Create the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 📊 Compute TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 📚 Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 🔍 Show results\n",
    "for doc_idx, doc_vector in enumerate(tfidf_matrix.toarray()):\n",
    "    print(f\"\\n📄 Document {doc_idx + 1}\")\n",
    "    keyword_scores = list(zip(feature_names, doc_vector))\n",
    "    top_keyword = max(keyword_scores, key=lambda x: x[1])\n",
    "    print(f\"Top keyword: '{top_keyword[0]}' (score: {top_keyword[1]:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
